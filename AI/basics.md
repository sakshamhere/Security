
`Predictive AI`: Predictive AI systems are designed to make predictions or classifications based on input data. Examples include fraud detection, image recognition, and recommendation systems.

Key Threats to Predictive AI:

- `Evasion Attacks`: These attacks occur when an attacker crafts inputs that mislead the model, causing it to perform its task incorrectly.

- `Model Theft`: In this attack, the modelâ€™s parameters or functionality are stolen. 

- `Model Poisoning`: This involves the manipulation of data, the data pipeline, or the model training supply chain during the training phase (development phase).


`Generative AI`: Generative AI systems produce outputs such as text, images, or audio. Examples include large language models (LLMs) like ChatGPT and large vision models (LVMs) like DALL-E and MidJourney.

Key Threats to Generative AI:

- `Prompt Injection`: In this type of attack, the attacker provides the model with manipulative instructions aimed at achieving malicious outcomes or objectives.

-` Direct Runtime Model Theft`: Attackers target parts of the model or critical components like the system prompt. 

- `Insecure Output Handling:` Generative AI systems can be vulnerable to traditional injection attacks, leading to risks if the outputs are improperly handled or processed.

# Prompt Engineering

What is a prompt?

Prompt contains everything:

- Context
- Instructions
- Input Data
- Output Indicator

What we can do using prompt?

- Summarization
- Extraction
- Inference/Classification
- Transformation
- Expansion
- Conversation